# NLP Fundamentals

This directory contains notebooks and code related to Natural Language Processing fundamentals, including text preprocessing, word embeddings, and text classification techniques.

## Contents

### 1. NLP_Text_Preprocessing_Techniques.ipynb
A comprehensive guide to text preprocessing techniques essential for NLP tasks, including:
- Tokenization
- Stopword removal
- Stemming and lemmatization
- Text normalization
- Special character handling

### 2. Word_Vector_Representations_Part1.ipynb
Introduction to word embeddings and vector representations:
- One-hot encoding
- Count vectorization
- TF-IDF vectorization
- Word2Vec basics

### 3. Advanced_Word_Embeddings_Part2.ipynb
Advanced word embedding techniques:
- Word2Vec (CBOW and Skip-gram)
- GloVe embeddings
- FastText
- Contextual embeddings introduction

### 4. Sentiment_Analysis_ML_Techniques.ipynb
Implementation of text classification for sentiment analysis using:
- Traditional ML approaches (Naive Bayes, SVM, etc.)
- Feature engineering for text data
- Model evaluation and performance metrics
- Hyperparameter tuning

## Usage
Each notebook can be run independently in Jupyter Notebook or Google Colab. Required libraries are specified at the beginning of each notebook.

## Author
Rohit Chigatapu - Computer Science Engineering Student