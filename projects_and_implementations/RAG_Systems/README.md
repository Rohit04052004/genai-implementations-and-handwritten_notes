# Retrieval Augmented Generation (RAG) Systems

This directory contains projects focused on implementing Retrieval Augmented Generation (RAG) systems, which enhance LLM outputs with external knowledge retrieval.

## Projects

### 1. Knowledge_Enhanced_LLM.zip
A comprehensive RAG implementation that includes:
- Document processing pipeline
- Chunking strategies and optimization
- Embedding generation and storage
- Vector database integration
- Query processing and context retrieval
- Response generation with retrieved context

**Key components:**
- Document loaders for various file types
- Text splitters with configurable parameters
- Embedding models comparison
- Vector store options (FAISS, Chroma, etc.)
- Retrieval strategies (similarity, MMR, etc.)
- LLM integration for response generation

### 2. Gemini_Enhanced_Retrieval.zip
A RAG implementation specifically optimized for Google's Gemini models:
- Gemini API integration
- Multimodal RAG capabilities
- Structured and unstructured data handling
- Performance optimization techniques
- Evaluation metrics and benchmarking

**Features:**
- Support for text, images, and structured data
- Advanced retrieval techniques
- Context window optimization
- Response quality evaluation
- Cost optimization strategies

## Requirements
- Python 3.8+
- LangChain or LlamaIndex
- Vector database (FAISS, Chroma, Pinecone, etc.)
- Embedding models
- LLM access (OpenAI, Gemini, etc.)
- Document processing libraries

## Setup
Extract the zip files and follow the README instructions within each project for detailed setup and usage guidelines.

## Resources
The projects include references to additional resources for RAG implementation, including:
- Research papers
- Online tutorials
- Community resources
- Best practices

## Author
Rohit Chigatapu - Computer Science Engineering Student